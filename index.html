<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Security Camera (Gemini) - Optimized</title>
    <style>
        body { font-family: sans-serif; background-color: #222; color: #eee; text-align: center; margin: 0; padding: 0; }
        #container { position: relative; width: 100%; max-width: 1280px; /* Increased for higher resolution */ margin: 0 auto; }
        #videoElement { width: 100%; display: block; border: 3px solid #4CAF50; }
        #overlayCanvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; pointer-events: none; }
        #status { margin-top: 10px; font-weight: bold; }
        .bounding-box { position: absolute; border: 2px solid yellow; box-sizing: border-box; }
        .label { position: absolute; top: -20px; left: 0; background-color: rgba(255, 255, 0, 0.7); color: black; padding: 2px 5px; font-size: 12px; }
    </style>
</head>
<body>
    <h1>AI Security Camera</h1>
    <div id="container">
        <video id="videoElement" autoplay muted playsinline></video>
        <canvas id="overlayCanvas"></canvas>
    </div>
    <div id="status">Starting...</div>

    <script>
        const TELEGRAM_BOT_TOKEN = '7785386019:AAHN8h2fje-njjcvAUUftx6TjJFE7rB4lvk';  // REPLACE
        const CHAT_ID = '8101021767';  // REPLACE
        const GEMINI_API_KEY = 'AIzaSyCuidd3GOSAck24jDAgFpdqZBnINsFA79k'; // Your Gemini API key
        const GEMINI_API_URL = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key=' + GEMINI_API_KEY;

        const video = document.getElementById('videoElement');
        const overlayCanvas = document.getElementById('overlayCanvas');
        const overlayCtx = overlayCanvas.getContext('2d');
        const statusDiv = document.getElementById('status');
        const cooldownPeriod = 1000; // Reduced to 1 second for faster human detection
        let lastCaptureTime = 0;
        let isProcessing = false;
        let animationFrameId = null; // Store the animation frame ID


        function handleError(message, error) {
            console.error(message, error);
            statusDiv.textContent = `Error: ${message}`;
            statusDiv.style.color = 'red';
        }


        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: "user",
                        width: { ideal: 1920 },  // Request higher resolution (up to 4K, if available)
                        height: { ideal: 1080 } // Adjust as needed for aspect ratio
                    },
                    audio: false
                });
                video.srcObject = stream;

                video.onloadedmetadata = () => {
                    // Set canvas size to match video size (for drawing).
                    overlayCanvas.width = video.videoWidth;
                    overlayCanvas.height = video.videoHeight;
                    statusDiv.textContent = "Camera ready.  Waiting for AI...";
                    // Start the processing loop *immediately* after metadata is loaded
                    processFrame();
                };
            } catch (err) {
                handleError("Could not access the camera", err);
            }
        }

        // Call Gemini API (REAL IMPLEMENTATION) - Optimized for speed
        async function callGeminiAPI(imageData) {
          try {
            const base64ImageData = imageDataToBytes(imageData);

            const requestBody = {
                contents: [{
                    parts: [
                        { text: "Detect humans in this image. Provide bounding boxes. Give concise results in this format:\n* **Human**: Confidence: 0.9, Bounding box: (0.1, 0.2, 0.3, 0.4)" }, // More concise prompt
                        { inline_data: { mime_type: "image/jpeg", data: base64ImageData } }
                    ]
                }]
            };

            const response = await fetch(GEMINI_API_URL, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(requestBody),
            });

            if (!response.ok) {
                throw new Error(`Gemini API error: ${response.status} - ${await response.text()}`);
            }

            const data = await response.json();
            const detections = parseGeminiResponse(data.candidates[0]?.content?.parts[0]?.text); // More concise access
            return detections;

          } catch (error) {
            handleError("Error calling/parsing Gemini API:", error);
            return [];
          }
        }

        // imageDataToBytes (Optimized for JPEG and quality)
        function imageDataToBytes(imageData) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            canvas.width = imageData.width;
            canvas.height = imageData.height;
            ctx.putImageData(imageData, 0, 0);
            // Use JPEG with a balance of quality and speed (0.7 is often a good compromise)
            const dataURL = canvas.toDataURL('image/jpeg', 0.7);
            return dataURL.split(',')[1];
        }

        // Simplified parsing (assuming consistent format)
        function parseGeminiResponse(responseText) {
             const detections = [];
            if (!responseText) {
                return detections;
            }

            const lines = responseText.split('\n');
            for (const line of lines) {
                if (line.startsWith("*")) {
                  const match = line.match(/\*\s*\*\*([^*]+)\*\*:\s*Confidence:\s*([\d.]+),\s*Bounding box:\s*\(([\d\.]+),\s*([\d\.]+),\s*([\d\.]+),\s*([\d\.]+)\)/);
                    if (match) {
                        const [, label, confidence, x1, y1, x2, y2] = match;
                        detections.push({
                            label: label.trim(),
                            confidence: parseFloat(confidence),
                            bbox: { x1: parseFloat(x1), y1: parseFloat(y1), x2: parseFloat(x2), y2: parseFloat(y2) }
                        });
                    }
                }
            }
            return detections;
        }

        // drawDetections (No changes needed for speed)
        function drawDetections(detections) {
           overlayCtx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

            for (const detection of detections) {
                const { x1, y1, x2, y2 } = detection.bbox;
                const x = x1 * overlayCanvas.width;
                const y = y1 * overlayCanvas.height;
                const width = (x2 - x1) * overlayCanvas.width;
                const height = (y2 - y1) * overlayCanvas.height;

                overlayCtx.strokeStyle = 'yellow';
                overlayCtx.lineWidth = 2;
                overlayCtx.strokeRect(x, y, width, height);

                overlayCtx.fillStyle = 'rgba(255, 255, 0, 0.7)';
                overlayCtx.fillRect(x, y - 20, width, 20);
                overlayCtx.fillStyle = 'black';
                overlayCtx.font = '12px sans-serif';
                overlayCtx.fillText(`${detection.label} (${(detection.confidence * 100).toFixed(1)}%)`, x + 5, y - 5);

                overlayCtx.fillStyle = "rgba(0, 0, 0, 0.3)";
                const dotSize = 2;
                for (let i = x; i < x + width; i += dotSize * 2) {
                    for (let j = y; j < y + height; j += dotSize * 2) {
                        overlayCtx.fillRect(i, j, dotSize, dotSize);
                    }
                }
            }
        }


        // processFrame (Optimized for speed and human detection)
       async function processFrame() {
            if (isProcessing) {
                animationFrameId = requestAnimationFrame(processFrame); // Keep requesting frames
                return;
            }
            isProcessing = true;

            overlayCtx.drawImage(video, 0, 0, overlayCanvas.width, overlayCanvas.height);
            const imageData = overlayCtx.getImageData(0, 0, overlayCanvas.width, overlayCanvas.height);

            const detections = await callGeminiAPI(imageData);
            drawDetections(detections);

            const humanDetected = detections.some(d => d.label.toLowerCase() === 'human' && d.confidence > 0.8);
            if (humanDetected) {
                const currentTime = Date.now();
                if (currentTime - lastCaptureTime > cooldownPeriod) {
                    statusDiv.textContent = "Human detected! Capturing...";
                    captureAndSendImage();
                    lastCaptureTime = currentTime;
                }
            } else {
                 statusDiv.textContent = 'Waiting for human...';
            }

            isProcessing = false;
            animationFrameId = requestAnimationFrame(processFrame); // Request the *next* frame
        }

        // captureAndSendImage (No changes needed for speed)
        async function captureAndSendImage() {
          overlayCanvas.toBlob(async (blob) => {
            if (!blob) {
                handleError("Failed to create image blob.");
                return;
            }

            const formData = new FormData();
            formData.append('chat_id', CHAT_ID);
            formData.append('photo', blob, 'security_capture.png');

            try {
                const response = await fetch(`https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}/sendPhoto`, { method: 'POST', body: formData });
                const data = await response.json();
                if (data.ok) {
                    statusDiv.textContent = `Image sent at ${new Date().toLocaleTimeString()}`;
                    statusDiv.style.color = 'green';
                } else {
                    handleError("Telegram API Error", data);
                }
            } catch (error) {
                handleError("Failed to send image.", error);
            }
          }, 'image/png');
        }



        // Initialize
        startCamera();

    </script>
</body>
</html>
